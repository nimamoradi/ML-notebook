{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words bag of words is one of the most basic ways to represent a word, it is simply a word counter. let us look at an example\n",
    "```\n",
    "1 :) John likes to watch movies. Mary likes movies too.\n",
    "\n",
    "2 :) John also likes to watch football games.\n",
    "```\n",
    "if we count the occurrence of each word\n",
    "```\n",
    "BoW1 = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1};\n",
    "\n",
    "BoW2 = {\"John\":1,\"also\":1,\"likes\":1,\"to\":1,\"watch\":1,\"football\":1,\"games\":1};\n",
    "```\n",
    "Ok let us see how to implement it in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset we use called 20 newsgroup it consist of 20 group with short text \n",
    "# scikit have builtin way to load the dataset just like iris and etc.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will only work with data section because bag of word don't need labels\n",
    "pprint(list(newsgroups_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first thing bag-of-word do is consist of a corpus of all existing word and make a one-hot vector for each of them.\n",
    "\n",
    "Now each we have a very large vector with one 1 in each row, we represent each sentence as a vector with for existing word it will show 1 in a corresponding bit and 0 not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = newsgroups_train.data\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(corpus).todense()\n",
    "\n",
    "print( take(10, vectorizer.vocabulary_.items()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  to demonstrate let us consider this corpus \n",
    "#  we have 24-word unique word with will be of vector size for each word and every sentence is the summation of its unique binary words\n",
    "corpus = [\n",
    "'All my cats in a row',\n",
    "'When my cat sits down, she looks like a Furby toy!',\n",
    "'The cats from outer space',\n",
    "'Sunshine loves to sit like this for some reason.'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print( vectorizer.fit_transform(corpus).todense() )\n",
    "print( vectorizer.vocabulary_ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocssing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in our above code model will consider cat and cats diffrent word and have may useless word in our bow\n",
    "\n",
    "####Stopwords \n",
    "Stopwords are a collection of common words like 'a','is','the'\n",
    "which would not contribute too much on the meaning of sentence \n",
    "####Stemming\n",
    "stemming is process of remove prefix and suffix form words, doing this make study and studing a single word,bear in mind that using lemmatization will lead to better result, but it's harder to impelement on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use stopwords list from nltk\n",
    "import nltk\n",
    "# download stop words if you do not have it with this command {nltk.download('stopwords')}\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def process(input_text):\n",
    "    # Create a regular expression tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Create a Snowball stemmer \n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # Get the list of stop words \n",
    "    stop_words =  nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # Tokenize the input string\n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "\n",
    "    # Remove the stop words \n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    \n",
    "    # Perform stemming on the tokenized words \n",
    "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "\n",
    "    return ' '.join(tokens_stemmed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,item in enumerate(newsgroups_train.data):\n",
    "    newsgroups_train.data[index] = process(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print( X_train.shape )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
